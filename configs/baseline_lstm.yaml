# RecurrentPPO LSTM baseline config for proper sequence handling
timesteps: 50000           # Moderate training for testing
env_id: FlyCraft-v0
run_name: baseline_lstm_recurrent
output_dir: runs
seed: 11
n_envs: 4                  # RecurrentPPO works well with fewer parallel envs

# Use RecurrentPPO instead of regular PPO
algorithm: RecurrentPPO
policy: MlpLstmPolicy      # Built-in RecurrentPPO policy

# RecurrentPPO-specific settings
recurrent_kwargs:
  lstm_hidden_size: 256    # LSTM hidden size
  n_lstm_layers: 2         # Number of LSTM layers
  shared_lstm: False       # Separate LSTMs for actor/critic
  enable_critic_lstm: True # Enable LSTM for critic

ppo_kwargs:
  learning_rate: 3.0e-4    # Standard learning rate
  n_steps: 128             # Shorter sequences for LSTM
  batch_size: 64           # Smaller batches for recurrent training
  n_epochs: 4              # Fewer epochs per update
  gamma: 0.99
  gae_lambda: 0.95
  ent_coef: 0.01
  vf_coef: 0.5
  clip_range: 0.2
  max_grad_norm: 0.5

# Evaluation and saving
save_freq: 5000
eval_freq: 2500
n_eval_episodes: 5

# Metrics and logging
log_metrics: true
csv_logging: true
tensorboard_logging: true
