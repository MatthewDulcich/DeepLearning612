# Large Transformer teacher configuration
env_id: FlyCraft-Nav-v0
run_name: teacher_large_revised
output_dir: runs
seed: 42
n_envs: 16
max_episode_steps: 1000

policy: transformer
policy_kwargs:
  transformer_kwargs:
    embed_dim: 512
    depth: 8
    num_heads: 16
    dropout: 0.1
    memory_size: 128

# ----------------- REVISED PPO HYPERPARAMETERS -----------------
# We are making the learning process more stable and efficient.
ppo_kwargs:
  # Reduced learning rate to prevent large, unstable updates.
  learning_rate: 5.0e-5 # Was 3.0e-4

  # Increased steps to collect more data per update, leading to a more stable gradient.
  n_steps: 4096 # Was 2048

  # Adjusted batch size to be a factor of n_steps for better data handling.
  batch_size: 1024 # Was 4096

  # Increased epochs to allow more passes over the collected data.
  n_epochs: 10 # Was 4

  gamma: 0.995
  gae_lambda: 0.95
  clip_range: 0.2

# Increased total timesteps to give the larger model more time to learn.
timesteps: 10000000 # Was 5000000
save_freq: 50000
eval_freq: 20000
n_eval_episodes: 10

use_spatio_temporal: true
predict_sequence: true
prediction_horizon: 200
eval:
  # primary thresholds
  ttc: 3.0
  path_dev: 0.5
  completion: 60.0
  latency: 10.0
  goal_radius: 0.6

  # tolerances for path deviation
  path_dev_tol_pct: 0.10
  path_dev_tol_abs: 0.00

  # tolerant numeric compare
  success_atol: 0.0
  success_rtol: 0.05
  success_reduce: "all"
  success_min_frac: 1.0