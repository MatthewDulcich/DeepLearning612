[DEBUG] Final policy_kwargs for lstm: {'lstm_hidden': 256, 'dropout': 0.1, 'num_layers': 2, 'log_std_init': -0.3}
Using cpu device
/home/mrdulcich/miniconda3/envs/drone-rl/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 3
We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.
Info: (n_steps=3 and n_envs=1)
  warnings.warn(
Logging to runs/baseline_lstm
[INFO] Weights & Biases logging enabled. RL metrics will be sent to wandb.
[INFO] Starting training for 10000 timesteps with policy: lstm
[INFO] Environment: FlyCraft-v0 | n_envs: 1 | Device: cpu
[INFO] Run directory: runs/baseline_lstm
Eval num_timesteps=1, episode_reward=-265.46 +/- 42.86
Episode length: 287.60 +/- 112.80
Success rate: 0.00%
---------------------------------------------
| eval/              |                      |
|    mean_ep_length  | 288                  |
|    mean_reward     | -265                 |
|    success_rate    | 0                    |
| model/             |                      |
|    extractor_type  | LSTMFeatureExtractor |
|    parameters      | 806669               |
| time/              |                      |
|    total_timesteps | 1                    |
---------------------------------------------
New best mean reward!
[ERROR] Exception during training: could not broadcast input array from shape (3,) into shape (1,)
Traceback (most recent call last):
  File "/home/mrdulcich/Desktop/DeepLearning612/src/drone_rl/train/train.py", line 523, in main
    model.learn(total_timesteps=timesteps, callback=callbacks)
  File "/home/mrdulcich/miniconda3/envs/drone-rl/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
  File "/home/mrdulcich/miniconda3/envs/drone-rl/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 324, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/mrdulcich/miniconda3/envs/drone-rl/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 247, in collect_rollouts
    rollout_buffer.add(
  File "/home/mrdulcich/miniconda3/envs/drone-rl/lib/python3.10/site-packages/stable_baselines3/common/buffers.py", line 797, in add
    self.log_probs[self.pos] = log_prob.clone().cpu().numpy()
ValueError: could not broadcast input array from shape (3,) into shape (1,)
[ERROR] Saving model checkpoint due to error: runs/baseline_lstm/checkpoints/checkpoint_error.zip
